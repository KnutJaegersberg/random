{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cc1400c",
   "metadata": {},
   "source": [
    "# How to use 30-B Galactica on a single GPU system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee0bb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/knut/paperml/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/knut/miniconda3/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/knut/paperml/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n",
      "Soverenity means, no cloud NSA or Trump reloaded surveillance in my reasoners, because <work>\n",
      "\n",
      "</work>\n",
      "\n",
      "Answer: No cloud NSA or Trump reloaded surveillance in my reasoners, because <work>\n",
      "\n",
      "##  What is the difference between the two?\n",
      "\n",
      "The difference between the two is that the first is a *consequence* of the second, but the second is not a consequence of the first.\n",
      "\n",
      "##  What is the difference between the two?\n",
      "\n",
      "The difference between the two is that the first is a *consequence* of the second, but the second is not a consequence of the first.\n",
      "\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "#First overwrite the function replace_8bit_linear in transformers/utils/bitsandbytes.py and import it I added\n",
    "#For further reference: https://github.com/huggingface/transformers/issues/20361\n",
    "#and: https://github.com/huggingface/transformers/pull/20281\n",
    "#import bitsandbytes as bnb\n",
    "#from torch import nn\n",
    "#from transformers.utils import bitsandbytes\n",
    "\n",
    "'''\n",
    "def replace_8bit_linear(parent_module, threshold=6.0, modules_to_not_convert=None, parent_layer_path=\"\"):\n",
    "    modules_to_not_convert = [\"lm_head\"] if modules_to_not_convert is None else modules_to_not_convert\n",
    "\n",
    "    parent_layer_prefix = \"\" if parent_layer_path == \"\" else parent_layer_path + \".\"\n",
    "    for name, module in parent_module.named_children():\n",
    "        layer_path = parent_layer_prefix + name\n",
    "\n",
    "        if layer_path in modules_to_not_convert:\n",
    "            continue\n",
    "\n",
    "        replace_8bit_linear(module, threshold, modules_to_not_convert, layer_path)\n",
    "\n",
    "        if isinstance(module, nn.Linear) and name not in modules_to_not_convert:\n",
    "            with bitsandbytes.init_empty_weights():\n",
    "                parent_module._modules[name] = bnb.nn.Linear8bitLt(\n",
    "                    module.in_features,\n",
    "                    module.out_features,\n",
    "                    module.bias is not None,\n",
    "                    has_fp16_weights=False,\n",
    "                    threshold=threshold,\n",
    "                )\n",
    "\n",
    "    return parent_module\n",
    "'''\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, OPTForCausalLM\n",
    "\n",
    "#switch as many to 'cpu' as necessary on your system \n",
    "\n",
    "device_map = {\n",
    "    \"model.decoder.embed_tokens\": 0,\n",
    "    \"model.decoder.embed_positions\": 0,\n",
    "    \"model.decoder.final_layer_norm\": 0,\n",
    "    \"lm_head\": 0,\n",
    "    \"model.decoder.layers.0\": 0,\n",
    "    \"model.decoder.layers.1\": 0,\n",
    "    \"model.decoder.layers.2\": 0,\n",
    "    \"model.decoder.layers.3\": 0,\n",
    "    \"model.decoder.layers.4\": 0,\n",
    "    \"model.decoder.layers.5\": 0,\n",
    "    \"model.decoder.layers.6\": 0,\n",
    "    \"model.decoder.layers.7\": 0,\n",
    "    \"model.decoder.layers.8\": 0,\n",
    "    \"model.decoder.layers.9\": 0,\n",
    "    \"model.decoder.layers.10\": 0,\n",
    "    \"model.decoder.layers.11\": 0,\n",
    "    \"model.decoder.layers.12\": 0,\n",
    "    \"model.decoder.layers.13\": 0,\n",
    "    \"model.decoder.layers.14\": 0,\n",
    "    \"model.decoder.layers.15\": 0,\n",
    "    \"model.decoder.layers.16\": 0,\n",
    "    \"model.decoder.layers.17\": 0,\n",
    "    \"model.decoder.layers.18\": 0,\n",
    "    \"model.decoder.layers.19\": 0,\n",
    "    \"model.decoder.layers.20\": 0,\n",
    "    \"model.decoder.layers.21\": 0,\n",
    "    \"model.decoder.layers.22\": 0,\n",
    "    \"model.decoder.layers.23\": 0,\n",
    "    \"model.decoder.layers.24\": 0,\n",
    "    \"model.decoder.layers.25\": 0,\n",
    "    \"model.decoder.layers.26\": 0,\n",
    "    \"model.decoder.layers.27\": 0,\n",
    "    \"model.decoder.layers.28\": 0,\n",
    "    \"model.decoder.layers.29\": 0,\n",
    "    \"model.decoder.layers.30\": 0,\n",
    "    \"model.decoder.layers.31\": \"cpu\",\n",
    "    \"model.decoder.layers.32\": \"cpu\",\n",
    "    \"model.decoder.layers.33\": \"cpu\",\n",
    "    \"model.decoder.layers.34\": \"cpu\",\n",
    "    \"model.decoder.layers.35\": \"cpu\",\n",
    "    \"model.decoder.layers.36\": \"cpu\",\n",
    "    \"model.decoder.layers.37\": \"cpu\",\n",
    "    \"model.decoder.layers.38\": \"cpu\",\n",
    "    \"model.decoder.layers.39\": \"cpu\",\n",
    "    \"model.decoder.layers.40\": \"cpu\",\n",
    "    \"model.decoder.layers.41\": \"cpu\",\n",
    "    \"model.decoder.layers.42\": \"cpu\",\n",
    "    \"model.decoder.layers.43\": \"cpu\",\n",
    "    \"model.decoder.layers.44\": \"cpu\",\n",
    "    \"model.decoder.layers.45\": \"cpu\",\n",
    "    \"model.decoder.layers.46\": \"cpu\",\n",
    "    \"model.decoder.layers.47\": \"cpu\",\n",
    "}\n",
    "\n",
    "model_kwargs = {\"device_map\": device_map, \"load_in_8bit\": True}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-30b\",  cache_dir=\"/run/media/knut/HD/language models/\", use_fast=False)\n",
    "layer_paths = [path for path, device in device_map.items() if device == \"cpu\"]\n",
    "skip=layer_paths + [\"lm_head\"]\n",
    "model = OPTForCausalLM.from_pretrained(\"facebook/galactica-30b\",  cache_dir=\"/run/media/knut/HD/language models/\", **model_kwargs, load_in_8bit_skip_modules=skip)\n",
    "input_text = \"Soverenity means, no cloud NSA or Trump reloaded surveillance in my reasoners, because <work>\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = model.generate(input_ids, min_length=100, max_length=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "028e052f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soverenity means, no cloud NSA or Trump reloaded surveillance in my reasoners, because <work>\n",
      "\n",
      "</work>\n",
      "\n",
      "Answer: A\n",
      "\n",
      "Question: What is the author's takeaway from the article?\n",
      "\n",
      "A. Sovereignty is an unattainable ideal.\n",
      "B. We shouldn't worry about sovereignty, because it's not our job.\n",
      "C. Sovereignty is a bad idea, but we need to work on it.\n",
      "D. Sovereignty is the best thing since sliced bread.\n",
      "\n",
      "<work>\n",
      "\n",
      "\"Sovereignty is an unattainable ideal.\"\n",
      "\n",
      "False. \"Sovereignty is a bad idea, but we need to work on it.\"\n",
      "\n",
      "False. \"Sovereignty is the best thing since sliced bread.\"\n",
      "\n",
      "Correct. </work>\n",
      "\n",
      "Answer\n"
     ]
    }
   ],
   "source": [
    "#again with contrastive search\n",
    "outputs = model.generate(input_ids, min_length=100, max_length=200, penalty_alpha=0.6, top_k=4)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b441425e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soverenity means, no cloud NSA or Trump reloaded surveillance in my reasoners, because <work>\n",
      "\n",
      "</work>\n",
      "\n",
      "Answer: No cloud NSA or Trump reloaded surveillance in my reasoners, because <work>\n",
      "\n",
      "## Part 2: The problem with the NSA\n",
      "\n",
      "The NSA is a very powerful organization. It has access to all of the internet traffic in the world. It has access to all of the phone calls in the world. It has access to all of the financial transactions in the world. It has access to all of the emails in the world. It has access to all of the documents in the world. It has access to all of the social media posts in the world. It has access to all of the web searches in the world. It has access to all of the video surveillance in the world. It has access to all of the audio surveillance in the world. It has access to all of the physical surveillance in the world. It\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Soverenity means, no cloud NSA or Trump reloaded surveillance in my reasoners, because \"\n",
    "#again with contrastive search\n",
    "outputs = model.generate(input_ids, min_length=100, max_length=200, penalty_alpha=0.1, top_k=4)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperml",
   "language": "python",
   "name": "paperml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
